{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb1225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INPUT DATA (Y) ---\n",
      "[[1 5 0]\n",
      " [1 0 2]\n",
      " [0 5 3]]\n",
      "------------------------------\n",
      "\n",
      "[STEP 3] Transactions:\n",
      "User 0: ['user_group_2', 'item_1', 'item_group_0']\n",
      "User 1: ['user_group_0', 'item_group_1', 'item_2']\n",
      "User 2: ['user_group_1', 'item_1', 'item_group_0']\n",
      "\n",
      "[STEP 4] FP-Growth & Generate Rules\n",
      "\n",
      ">>> FREQUENT ITEMSETS (Support ราย Subset) <<<\n",
      "    support                              itemsets\n",
      "0   +0.6667                        (item_group_0)\n",
      "7   +0.6667                (item_1, item_group_0)\n",
      "1   +0.6667                              (item_1)\n",
      "10  +0.3333  (user_group_2, item_1, item_group_0)\n",
      "16  +0.3333          (user_group_1, item_group_0)\n",
      "15  +0.3333                (user_group_1, item_1)\n",
      "14  +0.3333  (user_group_0, item_group_1, item_2)\n",
      "13  +0.3333                (user_group_0, item_2)\n",
      "12  +0.3333                (item_group_1, item_2)\n",
      "11  +0.3333          (user_group_0, item_group_1)\n",
      "9   +0.3333          (user_group_2, item_group_0)\n",
      "8   +0.3333                (user_group_2, item_1)\n",
      "6   +0.3333                        (user_group_1)\n",
      "5   +0.3333                              (item_2)\n",
      "4   +0.3333                        (item_group_1)\n",
      "3   +0.3333                        (user_group_0)\n",
      "2   +0.3333                        (user_group_2)\n",
      "17  +0.3333  (user_group_1, item_1, item_group_0)\n",
      "------------------------------\n",
      "\n",
      "Generated Rules:\n",
      "                     antecedents                   consequents  support  confidence\n",
      "0                       (item_1)                (item_group_0)  +0.6667     +1.0000\n",
      "1                 (item_group_0)                      (item_1)  +0.6667     +1.0000\n",
      "2                 (user_group_2)                      (item_1)  +0.3333     +1.0000\n",
      "3                       (item_1)                (user_group_2)  +0.3333     +0.5000\n",
      "4                 (user_group_2)                (item_group_0)  +0.3333     +1.0000\n",
      "5                 (item_group_0)                (user_group_2)  +0.3333     +0.5000\n",
      "6         (user_group_2, item_1)                (item_group_0)  +0.3333     +1.0000\n",
      "7   (user_group_2, item_group_0)                      (item_1)  +0.3333     +1.0000\n",
      "8         (item_1, item_group_0)                (user_group_2)  +0.3333     +0.5000\n",
      "9                 (user_group_2)        (item_1, item_group_0)  +0.3333     +1.0000\n",
      "10                      (item_1)  (user_group_2, item_group_0)  +0.3333     +0.5000\n",
      "11                (item_group_0)        (user_group_2, item_1)  +0.3333     +0.5000\n",
      "12                (user_group_0)                (item_group_1)  +0.3333     +1.0000\n",
      "13                (item_group_1)                (user_group_0)  +0.3333     +1.0000\n",
      "14                (item_group_1)                      (item_2)  +0.3333     +1.0000\n",
      "15                      (item_2)                (item_group_1)  +0.3333     +1.0000\n",
      "16                (user_group_0)                      (item_2)  +0.3333     +1.0000\n",
      "17                      (item_2)                (user_group_0)  +0.3333     +1.0000\n",
      "18  (user_group_0, item_group_1)                      (item_2)  +0.3333     +1.0000\n",
      "19        (user_group_0, item_2)                (item_group_1)  +0.3333     +1.0000\n",
      "20        (item_group_1, item_2)                (user_group_0)  +0.3333     +1.0000\n",
      "21                (user_group_0)        (item_group_1, item_2)  +0.3333     +1.0000\n",
      "22                (item_group_1)        (user_group_0, item_2)  +0.3333     +1.0000\n",
      "23                      (item_2)  (user_group_0, item_group_1)  +0.3333     +1.0000\n",
      "24                (user_group_1)                      (item_1)  +0.3333     +1.0000\n",
      "25                      (item_1)                (user_group_1)  +0.3333     +0.5000\n",
      "26                (user_group_1)                (item_group_0)  +0.3333     +1.0000\n",
      "27                (item_group_0)                (user_group_1)  +0.3333     +0.5000\n",
      "28        (user_group_1, item_1)                (item_group_0)  +0.3333     +1.0000\n",
      "29  (user_group_1, item_group_0)                      (item_1)  +0.3333     +1.0000\n",
      "30        (item_1, item_group_0)                (user_group_1)  +0.3333     +0.5000\n",
      "31                (user_group_1)        (item_1, item_group_0)  +0.3333     +1.0000\n",
      "32                      (item_1)  (user_group_1, item_group_0)  +0.3333     +0.5000\n",
      "33                (item_group_0)        (user_group_1, item_1)  +0.3333     +0.5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "class SignalGenerator:\n",
    "    def __init__(self, k_user=30, k_item=20, min_support=0.3, \n",
    "                 min_confidence=0.5, rating_percentile_threshold=0.8,\n",
    "                 random_state=42):\n",
    "        self.k_user = k_user\n",
    "        self.k_item = k_item\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.rating_percentile_threshold = rating_percentile_threshold\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.user_labels_ = None\n",
    "        self.item_labels_ = None\n",
    "        self.transaction_list_ = None\n",
    "        self.rules_ = None\n",
    "        self.frequent_itemsets_ = None # <--- MODIFIED: เพิ่มตัวแปรสำหรับเก็บ Itemsets\n",
    "        self.num_users_ = None\n",
    "        self.num_items_ = None\n",
    "\n",
    "    def _run_clustering(self, Y: np.ndarray):\n",
    "        user_labels = KMeans(n_clusters=self.k_user, random_state=self.random_state, n_init=10).fit_predict(Y)\n",
    "        item_labels = KMeans(n_clusters=self.k_item, random_state=self.random_state, n_init=10).fit_predict(Y.T)\n",
    "        return user_labels, item_labels\n",
    "\n",
    "    def _binarize_ratings(self, Y: np.ndarray) -> np.ndarray:\n",
    "        Y_masked = np.where(Y > 0, Y, np.nan)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            min_ratings = np.nanmin(Y_masked, axis=1, keepdims=True)\n",
    "            max_ratings = np.nanmax(Y_masked, axis=1, keepdims=True)\n",
    "        min_ratings = np.nan_to_num(min_ratings, nan=0.0)\n",
    "        max_ratings = np.nan_to_num(max_ratings, nan=1.0)\n",
    "        max_ratings[max_ratings == min_ratings] = min_ratings[max_ratings == min_ratings] + 1.0 \n",
    "        thresholds_per_user = min_ratings + (max_ratings - min_ratings) * self.rating_percentile_threshold\n",
    "        return (Y >= thresholds_per_user)\n",
    "\n",
    "    def _create_transaction_list(self, transactions_binarized: np.ndarray):\n",
    "        transaction_list = []\n",
    "        for u_idx in range(self.num_users_):\n",
    "            trans = {f'user_group_{self.user_labels_[u_idx]}'}\n",
    "            liked_items = np.where(transactions_binarized[u_idx])[0] \n",
    "            if len(liked_items) > 0:\n",
    "                for i_idx in liked_items:\n",
    "                    trans.add(f'item_{i_idx}')\n",
    "                    trans.add(f'item_group_{self.item_labels_[i_idx]}')\n",
    "                transaction_list.append(list(trans))\n",
    "        return transaction_list\n",
    "\n",
    "    def _find_association_rules(self):\n",
    "        if not self.transaction_list_: return pd.DataFrame()\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(self.transaction_list_).transform(self.transaction_list_)\n",
    "        df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        \n",
    "        # หา Frequent Itemsets\n",
    "        frequent_itemsets = fpgrowth(df, min_support=self.min_support, use_colnames=True)\n",
    "        \n",
    "        # <--- MODIFIED: เก็บค่า Itemsets ไว้ใน Class attribute\n",
    "        self.frequent_itemsets_ = frequent_itemsets \n",
    "        \n",
    "        if frequent_itemsets.empty: return pd.DataFrame()\n",
    "        \n",
    "        # สร้าง Rules ต่อ\n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=self.min_confidence)\n",
    "        return rules\n",
    "\n",
    "    def _filter_rules(self):\n",
    "        if self.rules_.empty: return self.rules_\n",
    "        filtered_rules = self.rules_[~self.rules_['consequents'].apply(lambda x: any(str(i).startswith('user_group_') for i in x))]\n",
    "        return filtered_rules\n",
    "\n",
    "    def _build_signal_matrix(self, Y: np.ndarray):\n",
    "        matrix_signal = np.zeros_like(Y, dtype=float)\n",
    "        item_map = {f'item_{i}': i for i in range(self.num_items_)}\n",
    "        item_group_map = {f'item_group_{i}': i for i in range(self.k_item)}\n",
    "        item_to_users = defaultdict(set)\n",
    "        for user_idx, history in enumerate(self.transaction_list_):\n",
    "            for item in history:\n",
    "                item_to_users[item].add(user_idx)\n",
    "        for antecedents_set, group_df in self.rules_.groupby('antecedents'):\n",
    "            antecedents = list(antecedents_set) \n",
    "            if not antecedents: continue\n",
    "            try:\n",
    "                matching_users = item_to_users.get(antecedents[0], set()).copy()\n",
    "                for item in antecedents[1:]:\n",
    "                    matching_users.intersection_update(item_to_users.get(item, set()))\n",
    "            except Exception as e: continue\n",
    "            if not matching_users: continue\n",
    "            target_user_indices = list(matching_users)\n",
    "            for _, rule in group_df.iterrows():\n",
    "                for consequent in rule['consequents']:\n",
    "                    if consequent in item_map:\n",
    "                        col_idx = item_map[consequent] \n",
    "                        matrix_signal[target_user_indices, col_idx] += rule['confidence']\n",
    "                    elif consequent in item_group_map:\n",
    "                        target_items = np.where(self.item_labels_ == item_group_map[consequent])[0]\n",
    "                        if target_items.size == 0: continue\n",
    "                        rows, cols = np.meshgrid(target_user_indices, target_items, indexing='ij')\n",
    "                        matrix_signal[rows, cols] += rule['confidence']\n",
    "        return matrix_signal\n",
    "\n",
    "# --- Demo Execution Code (Modified to show Itemsets) ---\n",
    "\n",
    "def run_debug_steps():\n",
    "    # 0. Setup Data & Model\n",
    "    Y = np.array([[1, 5, 0],\n",
    "                  [1, 0, 2],\n",
    "                  [0, 5, 3]])\n",
    "    \n",
    "    # ลด min_support ลงเล็กน้อยเพื่อให้เห็น itemsets มากขึ้นในข้อมูลตัวอย่าง\n",
    "    model = SignalGenerator(k_user=3, k_item=2, \n",
    "                            min_support=0.3, # <--- ปรับลดเพื่อให้เห็นตัวอย่างชัดขึ้น\n",
    "                            min_confidence=0.5, \n",
    "                            rating_percentile_threshold=0.8, random_state=42)\n",
    "    \n",
    "    print(\"--- INPUT DATA (Y) ---\")\n",
    "    print(Y)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    model.num_users_, model.num_items_ = Y.shape\n",
    "\n",
    "    # Step 1: Clustering\n",
    "    model.user_labels_, model.item_labels_ = model._run_clustering(Y)\n",
    "\n",
    "    # Step 2: Binarization\n",
    "    binarized_Y = model._binarize_ratings(Y)\n",
    "\n",
    "    # Step 3: Transaction List\n",
    "    model.transaction_list_ = model._create_transaction_list(binarized_Y)\n",
    "    print(\"\\n[STEP 3] Transactions:\")\n",
    "    for i, trans in enumerate(model.transaction_list_):\n",
    "        print(f\"User {i}: {trans}\")\n",
    "\n",
    "    # Step 4: Association Rules (With Itemsets View)\n",
    "    print(\"\\n[STEP 4] FP-Growth & Generate Rules\")\n",
    "    model.rules_ = model._find_association_rules()\n",
    "    \n",
    "    # <--- MODIFIED: แสดง Frequent Itemsets (Support ราย Subset)\n",
    "    print(\"\\n>>> FREQUENT ITEMSETS (Support ราย Subset) <<<\")\n",
    "    if model.frequent_itemsets_ is not None and not model.frequent_itemsets_.empty:\n",
    "        # เรียงลำดับตาม Support จากมากไปน้อยเพื่อให้อ่านง่าย\n",
    "        print(model.frequent_itemsets_.sort_values(by='support', ascending=False))\n",
    "    else:\n",
    "        print(\"No frequent itemsets found.\")\n",
    "    print(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "    if not model.rules_.empty:\n",
    "        print(\"Generated Rules:\")\n",
    "        print(model.rules_[['antecedents', 'consequents', 'support', 'confidence']])\n",
    "    else:\n",
    "        print(\"No rules found.\")\n",
    "\n",
    "# Execute\n",
    "run_debug_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "177fc5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: 1 Epochs, Batch Size: 1\n",
      "============================================================\n",
      "\n",
      ">>> EPOCH 1 START <<<\n",
      "\n",
      "----------------------------------------\n",
      "[Step 1] Processing User Index: 0\n",
      " 1. Current Vectors (Before Update):\n",
      "    - User P[0]: [-0.0077 -0.0075]\n",
      "    - Item Q (All): \n",
      "[[ 0.0028  0.0006]\n",
      " [ 0.0052 -0.0024]\n",
      " [-0.0005  0.0053]]\n",
      " 2. Calculation:\n",
      "    - Target Y: [1. 5. 0.]\n",
      "    - Pred   Y: [-0. -0. -0.]\n",
      "    - Error (Target - Pred): [1. 5. 0.]\n",
      "    - Loss Value: 26.240517\n",
      " 3. Gradients (Slope computed from Loss):\n",
      "    - Grad P[0]: [-0.0588  0.0167]\n",
      "    - Grad Q (All): \n",
      "[[ 0.0153  0.015 ]\n",
      " [ 0.0766  0.0751]\n",
      " [-0.     -0.    ]]\n",
      "      (Note: Grad Q will be close to 0 for items not rated by this user)\n",
      " 4. Updated Vectors:\n",
      "    - New User P[0]: [ 0.0023 -0.0175]\n",
      "\n",
      "----------------------------------------\n",
      "[Step 2] Processing User Index: 1\n",
      " 1. Current Vectors (Before Update):\n",
      "    - User P[1]: [ 0.0035 -0.0031]\n",
      "    - Item Q (All): \n",
      "[[-0.0072 -0.0094]\n",
      " [-0.0048 -0.0124]\n",
      " [ 0.0093 -0.0047]]\n",
      " 2. Calculation:\n",
      "    - Target Y: [1. 0. 2.]\n",
      "    - Pred   Y: [0. 0. 0.]\n",
      "    - Error (Target - Pred): [ 1. -0.  2.]\n",
      "    - Loss Value: 5.239998\n",
      " 3. Gradients (Slope computed from Loss):\n",
      "    - Grad P[1]: [-0.0227  0.037 ]\n",
      "    - Grad Q (All): \n",
      "[[-0.0071  0.0062]\n",
      " [ 0.     -0.    ]\n",
      " [-0.0141  0.0125]]\n",
      "      (Note: Grad Q will be close to 0 for items not rated by this user)\n",
      " 4. Updated Vectors:\n",
      "    - New User P[1]: [ 0.011  -0.0106]\n",
      "\n",
      "----------------------------------------\n",
      "[Step 3] Processing User Index: 2\n",
      " 1. Current Vectors (Before Update):\n",
      "    - User P[2]: [ 0.003  -0.0067]\n",
      "    - Item Q (All): \n",
      "[[-0.0101 -0.0185]\n",
      " [-0.0115 -0.0191]\n",
      " [ 0.0167 -0.0122]]\n",
      " 2. Calculation:\n",
      "    - Target Y: [0. 5. 3.]\n",
      "    - Pred   Y: [1.e-04 1.e-04 1.e-04]\n",
      "    - Error (Target - Pred): [-1.0000e-04  4.9999e+00  2.9999e+00]\n",
      "    - Loss Value: 34.238472\n",
      " 3. Gradients (Slope computed from Loss):\n",
      "    - Grad P[2]: [0.0249 0.2694]\n",
      "    - Grad Q (All): \n",
      "[[ 0.     -0.    ]\n",
      " [-0.0296  0.0667]\n",
      " [-0.0178  0.04  ]]\n",
      "      (Note: Grad Q will be close to 0 for items not rated by this user)\n",
      " 4. Updated Vectors:\n",
      "    - New User P[2]: [-0.0034 -0.0131]\n",
      "\n",
      ">>> EPOCH 1 END. Avg Loss: 21.9063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Dataset Class ---\n",
    "class PrepareDataset(Dataset):\n",
    "    def __init__(self, Y_tensor, S_tensor):\n",
    "        self.Y = Y_tensor\n",
    "        self.S = S_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "\n",
    "    def __getitem__(self, user_idx):\n",
    "        return user_idx, self.Y[user_idx], self.S[user_idx]\n",
    "\n",
    "# --- 2. CARMS Model (Enhanced Debugging) ---\n",
    "class CARMS_MF(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, user_count, item_count, K, learning_rate, lambda_rate, gamma):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        self.user_count = user_count\n",
    "        self.item_count = item_count\n",
    "        self.K = K\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_rate = lambda_rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Embeddings\n",
    "        self.P_embedding = torch.nn.Embedding(self.user_count, self.K) # User Latent\n",
    "        self.Q_embedding = torch.nn.Embedding(self.item_count, self.K) # Item Latent for Y\n",
    "        self.R_embedding = torch.nn.Embedding(self.item_count, self.K) # Item Latent for S\n",
    "\n",
    "        # Init Weights\n",
    "        nn.init.normal_(self.P_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.Q_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.R_embedding.weight, std=0.01)\n",
    "\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def forward(self, user_indices):\n",
    "        user_factors = self.P_embedding(user_indices)\n",
    "        item_factors_y = self.Q_embedding.weight\n",
    "        item_factors_s = self.R_embedding.weight\n",
    "        \n",
    "        # Matrix Multiplication\n",
    "        y_prediction = (user_factors @ item_factors_y.T)\n",
    "        s_prediction = (user_factors @ item_factors_s.T)\n",
    "        \n",
    "        return y_prediction, s_prediction\n",
    "\n",
    "    def fit(self, Y, S, epochs, batch_size=1):\n",
    "        self.to(self.device)\n",
    "        self.train()\n",
    "\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "        S_tensor = torch.tensor(S, dtype=torch.float32)\n",
    "\n",
    "        dataset = PrepareDataset(Y_tensor, S_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.lambda_rate\n",
    "        )\n",
    "        \n",
    "        print(f\"Start Training: {epochs} Epochs, Batch Size: {batch_size}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            print(f\"\\n>>> EPOCH {epoch+1} START <<<\")\n",
    "            \n",
    "            step = 1\n",
    "            for user_indices, Y_batch, S_batch in dataloader:\n",
    "                u_idx = user_indices.item()\n",
    "                print(f\"\\n\" + \"-\"*40)\n",
    "                print(f\"[Step {step}] Processing User Index: {u_idx}\")\n",
    "\n",
    "                user_indices = user_indices.to(self.device)\n",
    "                Y_batch = Y_batch.to(self.device)\n",
    "                S_batch = S_batch.to(self.device)\n",
    "                \n",
    "                # --- A. Show Current Vectors (Before Update) ---\n",
    "                # ดึงค่า weight ปัจจุบันออกมาดู (ใช้ .detach() เพื่อไม่ให้กระทบ graph)\n",
    "                curr_P = self.P_embedding.weight.data[u_idx].numpy().round(4)\n",
    "                # สำหรับ Q และ R ขอแสดงแค่ 2 items แรกพอสังเขป (หรือแสดงหมดถ้า item น้อย)\n",
    "                curr_Q = self.Q_embedding.weight.data.numpy().round(4) \n",
    "                \n",
    "                print(f\" 1. Current Vectors (Before Update):\")\n",
    "                print(f\"    - User P[{u_idx}]: {curr_P}\")\n",
    "                print(f\"    - Item Q (All): \\n{curr_Q}\")\n",
    "\n",
    "                # --- B. Forward & Error Calc ---\n",
    "                optimizer.zero_grad() # Clear gradients\n",
    "                y_hat, s_hat = self(user_indices)\n",
    "                \n",
    "                # Calculate Raw Error (e_ui = y_ui - y_hat)\n",
    "                # Note: MSE Loss จะยกกำลังสองค่านี้ แต่ Gradient จะมาจากค่านี้โดยตรง\n",
    "                raw_error_y = (Y_batch - y_hat).detach().numpy().flatten().round(4)\n",
    "                \n",
    "                print(f\" 2. Calculation:\")\n",
    "                print(f\"    - Target Y: {Y_batch.numpy().flatten()}\")\n",
    "                print(f\"    - Pred   Y: {y_hat.detach().numpy().flatten().round(4)}\")\n",
    "                print(f\"    - Error (Target - Pred): {raw_error_y}\")\n",
    "\n",
    "                # --- C. Loss Calculation ---\n",
    "                mask_y = Y_batch > 0\n",
    "                loss_y = self.loss_fn(y_hat[mask_y], Y_batch[mask_y])\n",
    "                \n",
    "                mask_s = S_batch != 0\n",
    "                loss_s = self.loss_fn(s_hat[mask_s], S_batch[mask_s])\n",
    "                \n",
    "                loss = loss_y + (self.gamma * loss_s)\n",
    "                \n",
    "                print(f\"    - Loss Value: {loss.item():.6f}\")\n",
    "\n",
    "                # --- D. Backward (Compute Gradients) ---\n",
    "                loss.backward()\n",
    "                \n",
    "                # --- E. Show Gradients ---\n",
    "                # Gradient จะถูกเก็บไว้ใน .grad ของ parameters\n",
    "                grad_P = self.P_embedding.weight.grad[u_idx].numpy().round(4)\n",
    "                grad_Q = self.Q_embedding.weight.grad.numpy().round(4)\n",
    "\n",
    "                print(f\" 3. Gradients (Slope computed from Loss):\")\n",
    "                print(f\"    - Grad P[{u_idx}]: {grad_P}\")\n",
    "                print(f\"    - Grad Q (All): \\n{grad_Q}\")\n",
    "                print(f\"      (Note: Grad Q will be close to 0 for items not rated by this user)\")\n",
    "\n",
    "                # --- F. Update Weights ---\n",
    "                # ใช้ Adam Optimizer ปรับค่า Weight\n",
    "                # หมายเหตุ: ค่าที่เปลี่ยนจะไม่เท่ากับ lr * grad เป๊ะๆ เพราะ Adam มี momentum\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=10.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                new_P = self.P_embedding.weight.data[u_idx].numpy().round(4)\n",
    "                print(f\" 4. Updated Vectors:\")\n",
    "                print(f\"    - New User P[{u_idx}]: {new_P}\")\n",
    "                \n",
    "                step += 1\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"\\n>>> EPOCH {epoch+1} END. Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# --- 3. Execution ---\n",
    "\n",
    "# Data Setup (เหมือนเดิม)\n",
    "Y = np.array([[1, 5, 0],\n",
    "              [1, 0, 2],\n",
    "              [0, 5, 3]])\n",
    "\n",
    "S = np.array([[0.0, 0.6931, 0.0],\n",
    "              [0.0, 0.0, 0.6931],\n",
    "              [0.0, 0.6931, 0.0]])\n",
    "\n",
    "# Model Parameters\n",
    "K = 2 \n",
    "learning_rate = 0.01 \n",
    "lambda_rate = 0.001\n",
    "gamma = 0.5 \n",
    "\n",
    "model = CARMS_MF(user_count=3, item_count=3, K=K, \n",
    "                 learning_rate=learning_rate, lambda_rate=lambda_rate, gamma=gamma)\n",
    "\n",
    "# Run Fit\n",
    "model.fit(Y, S, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744dcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
